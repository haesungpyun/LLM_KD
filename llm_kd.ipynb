{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n",
    "# from transformers.trainer_callback import DEFAULT_CALLBACKS, CallbackHandler\n",
    "from transformers.integrations import get_reporting_integration_callbacks\n",
    "from transformers.utils import find_labels\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor, StoppingCriteriaList, MaxLengthCriteria,\n",
    ")\n",
    "\n",
    "from logging import getLogger\n",
    "\n",
    "from LLM_KD.data import DataWrapper\n",
    "from LLM_KD.data_collator import Datacollator\n",
    "from LLM_KD.optimizer_wrapper import OptimizerWrapper\n",
    "from LLM_KD.scheduler_wrapper import SchedulerWrapper\n",
    "from LLM_KD.utils import count_num_examples, prepare_input, get_model_param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = getLogger(__name__)\n",
    "\n",
    "config = {\n",
    "        'output_dir': 'my_awesome_model',\n",
    "        'num_train_epochs': 3,\n",
    "        'learning_rate': 2e-5,\n",
    "        'train_batch_size': 16, \n",
    "        'valid_batch_size': 16, \n",
    "        'weight_decay': 0.01,\n",
    "        'model_name': 't5-small'\n",
    "    }\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config[\"model_name\"])\n",
    "model = model.to(device)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
    "\n",
    "# Load data_collator \n",
    "data_collator = Datacollator(tokenizer=tokenizer, model=config[\"model_name\"])\n",
    "\n",
    "# label name\n",
    "label_names = find_labels(model.__class__)\n",
    "\n",
    "# Data Wrapper\n",
    "data_wrapper = DataWrapper(\n",
    "        config=config, model=model, tokenizer=tokenizer, \n",
    "        data_collator=data_collator, label_names=label_names\n",
    ")\n",
    "\n",
    "# Load train, test dataset \n",
    "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
    "dataset['train'] = dataset['train'].select(range(1000))\n",
    "\n",
    "# Preprocess data\n",
    "tokenized  = dataset.map(data_wrapper.preprocess_function, batched=True)\n",
    "\n",
    "# Train, Valid, Test data\n",
    "train_data, valid_data, test_data = tokenized[\"train\"], tokenized[\"validation\"], tokenized[\"test\"]\n",
    "\n",
    "\n",
    "# with open('tokenized_train.pkl', 'rb') as f:\n",
    "#     train_data = pickle.load(f)[:1000]\n",
    "\n",
    "# with open('tokenized_train.pkl', 'rb') as f:\n",
    "#     valid_data = pickle.load(f)\n",
    "\n",
    "# with open('tokenized_train.pkl', 'rb') as f:\n",
    "#     test_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# Load Metric\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "# Data Collator\n",
    "default_collator = default_data_collator if tokenizer is None else DataCollatorWithPadding(tokenizer)\n",
    "data_collator = data_collator if data_collator is not None else default_collator\n",
    "\n",
    "# Optimizer, lr_scheduler\n",
    "optimizer, lr_scheduler = None, None\n",
    "\n",
    "# Callbacks\n",
    "# callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(None)\n",
    "# callback_handler = CallbackHandler(callbacks, model, tokenizer, optimizer, lr_scheduler)\n",
    "\n",
    "# Train dataloader\n",
    "train_dataloader = data_wrapper.get_train_dataloader(train_data)\n",
    "\n",
    "# Train batch size \n",
    "total_train_batch_size = config.get(\"train_batch_size\", 16) * config.get(\"gradient_accumulation_steps\", 1) * config.get(\"world_size\",1)\n",
    "\n",
    "# Number of update steps per epoch\n",
    "len_dataloader = len(train_dataloader)\n",
    "num_update_steps_per_epoch = max((len_dataloader // config.get(\"gradient_accumulation_steps\", 1)), 1)\n",
    "num_examples = count_num_examples(config, train_dataloader)\n",
    "\n",
    "# Total number of training steps\n",
    "max_steps = math.ceil(config.get('num_train_epochs', 3.0) * num_update_steps_per_epoch)\n",
    "num_train_epochs = math.ceil(config.get('num_train_epochs', 3.0))\n",
    "num_train_samples = num_examples * config.get('num_train_epochs', 3.0)\n",
    "\n",
    "# Optimizer, lr_scheduler\n",
    "optimizer = OptimizerWrapper(model, config).create_optimizer()\n",
    "lr_scheduler = SchedulerWrapper(config, optimizer, num_training_steps=max_steps).create_scheduler()\n",
    "\n",
    "# Train!\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {num_examples:,}\")\n",
    "logger.info(f\"  Num Epochs = {num_train_epochs:,}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {config.get('per_device_train_batch_size', 8):,}\")\n",
    "if config.get('per_device_train_batch_size', 8) != config.get('train_batch_size'):\n",
    "    logger.info(f\"  Training with DataParallel so batch size has been adjusted to: {config.get('train_batch_size'):,}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size:,}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {config.get('gradient_accumulation_steps',1)}\")\n",
    "logger.info(f\"  Total optimization steps = {max_steps:,}\")\n",
    "logger.info(f\"  Number of trainable parameters = {get_model_param_count(model, trainable_only=True):,}\")\n",
    "\n",
    "epochs_trained = 0  \n",
    "start_time = time.time()\n",
    "epochs_trained = 0\n",
    "steps_trained_in_current_epoch = 0\n",
    "total_batched_samples = 0\n",
    "tr_loss = torch.tensor(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs_trained, num_train_epochs):\n",
    "    epoch_iterator = train_dataloader\n",
    "    if hasattr(epoch_iterator, \"set_epoch\"):\n",
    "        epoch_iterator.set_epoch(epoch)\n",
    "\n",
    "    steps_in_epoch = (\n",
    "        len(epoch_iterator)\n",
    "        if len_dataloader is not None\n",
    "        else config.get('max_steps', -1) * config.get(\"gradient_accumulation_steps\",1)\n",
    "    )\n",
    "    \n",
    "    step = -1\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        total_batched_samples += 1\n",
    "        \n",
    "        model.train()\n",
    "        inputs = prepare_input(config, inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "\n",
    "        labels = None\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # define decoder start token ids\n",
    "        decoder_input = torch.ones((1, 1), device=model.device, dtype=torch.long)\n",
    "        decoder_input = decoder_input * model.config.decoder_start_token_id\n",
    "\n",
    "        # add encoder_outputs to model keyword arguments\n",
    "        model_kwargs = {\n",
    "            \"encoder_outputs\": model.get_encoder()(\n",
    "                inputs['input_ids'], return_dict=True\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # instantiate logits processors\n",
    "        logits_processor = LogitsProcessorList(\n",
    "            [\n",
    "                MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
    "        # break\n",
    "        outputs = model.greedy_search(decoder_input, logits_processor=logits_processor, stopping_criteria=stopping_criteria, **model_kwargs)\n",
    "        \n",
    "        tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        \n",
    "        tr_loss += loss        \n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        if not isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        model.zero_grad()\n",
    "    break\n",
    "\n",
    "logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llm/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (38) must match the size of tensor b (19) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/haesung/LLM_KD/llm_kd.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmlc-1/home/haesung/LLM_KD/llm_kd.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m logits_processor \u001b[39m=\u001b[39m LogitsProcessorList(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmlc-1/home/haesung/LLM_KD/llm_kd.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmlc-1/home/haesung/LLM_KD/llm_kd.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m         MinLengthLogitsProcessor(\u001b[39m5\u001b[39m, eos_token_id\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmlc-1/home/haesung/LLM_KD/llm_kd.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmlc-1/home/haesung/LLM_KD/llm_kd.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmlc-1/home/haesung/LLM_KD/llm_kd.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m stopping_criteria \u001b[39m=\u001b[39m StoppingCriteriaList([MaxLengthCriteria(max_length\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmlc-1/home/haesung/LLM_KD/llm_kd.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgreedy_search(input_ids, logits_processor\u001b[39m=\u001b[39;49mlogits_processor, stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmlc-1/home/haesung/LLM_KD/llm_kd.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/transformers/generation/utils.py:2522\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2519\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2521\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2522\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2523\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2524\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2525\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2526\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2527\u001b[0m )\n\u001b[1;32m   2529\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2530\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1745\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1747\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1748\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1749\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1750\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1751\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1752\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1753\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1754\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1755\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1756\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1757\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1758\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1759\u001b[0m )\n\u001b[1;32m   1761\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1763\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1113\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1099\u001b[0m         layer_module\u001b[39m.\u001b[39mforward,\n\u001b[1;32m   1100\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         output_attentions,\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[1;32m   1112\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1114\u001b[0m         hidden_states,\n\u001b[1;32m   1115\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1116\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1117\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1118\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1119\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1120\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1121\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1122\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1123\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1124\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1125\u001b[0m     )\n\u001b[1;32m   1127\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:724\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     query_length \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 724\u001b[0m cross_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m1\u001b[39;49m](\n\u001b[1;32m    725\u001b[0m     hidden_states,\n\u001b[1;32m    726\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    727\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    728\u001b[0m     position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m    729\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m    730\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[1;32m    731\u001b[0m     query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[1;32m    732\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    733\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    734\u001b[0m )\n\u001b[1;32m    735\u001b[0m hidden_states \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    737\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:635\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    624\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    633\u001b[0m ):\n\u001b[1;32m    634\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 635\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEncDecAttention(\n\u001b[1;32m    636\u001b[0m         normed_hidden_states,\n\u001b[1;32m    637\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    638\u001b[0m         key_value_states\u001b[39m=\u001b[39;49mkey_value_states,\n\u001b[1;32m    639\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    640\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    641\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    642\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    643\u001b[0m         query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[1;32m    644\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    645\u001b[0m     )\n\u001b[1;32m    646\u001b[0m     layer_output \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[1;32m    647\u001b[0m     outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/llm/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:560\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     position_bias_masked \u001b[39m=\u001b[39m position_bias\n\u001b[0;32m--> 560\u001b[0m scores \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_bias_masked\n\u001b[1;32m    561\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(scores\u001b[39m.\u001b[39mfloat(), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtype_as(\n\u001b[1;32m    562\u001b[0m     scores\n\u001b[1;32m    563\u001b[0m )  \u001b[39m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    564\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(\n\u001b[1;32m    565\u001b[0m     attn_weights, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining\n\u001b[1;32m    566\u001b[0m )  \u001b[39m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (38) must match the size of tensor b (19) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    ")\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "encoder_input_str = [\"translate English to German: How old are you, you little pretty young tall fat ?\",\n",
    "                     \"translate English to German: I am 20 years old.\"]\n",
    "encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\", padding=True).input_ids\n",
    "\n",
    "# define decoder start token ids\n",
    "input_ids = torch.ones((1, 1), device=model.device, dtype=torch.long)\n",
    "input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "# add encoder_outputs to model keyword arguments\n",
    "model_kwargs = {\n",
    "    \"encoder_outputs\": model.get_encoder()(\n",
    "        encoder_input_ids, return_dict=True\n",
    "    )\n",
    "}\n",
    "\n",
    "# instantiate logits processors\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
    "\n",
    "outputs = model.greedy_search(input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria, **model_kwargs)\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.input_ids.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.ones((1, 1), device=model.device, dtype=torch.long)\n",
    "input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "encoder_input_str = \"translate English to German: How old are you, you little pretty young tall fat ?\"\n",
    "encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# define decoder start token ids\n",
    "input_ids = torch.ones((1, 1), device=model.device, dtype=torch.long)\n",
    "input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "# add encoder_outputs to model keyword arguments\n",
    "model_kwargs = {\n",
    "    \"encoder_outputs\": model.get_encoder()(\n",
    "        encoder_input_ids, return_dict=True\n",
    "    )\n",
    "}\n",
    "\n",
    "# instantiate logits processors\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
    "\n",
    "outputs = model.greedy_search(input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria, **model_kwargs)\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wie alt bist du, du ein kleiner ziemlich junger großer fetter Kerl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_encoder()(encoder_input_ids, return_dict=True\n",
    ").last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# set pad_token_id to eos_token_id because GPT2 does not have a PAD token\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "input_prompt = \"It might be possible to\"\n",
    "input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# instantiate logits processors\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),\n",
    "    ]\n",
    ")\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
    "\n",
    "outputs = model.greedy_search(\n",
    "    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria\n",
    ")\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
