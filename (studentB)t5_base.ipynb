{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPa6g3Fj2tx7k/2CTkR+xCm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haesungpyun/LLM_KD/blob/Lin/(studentB)t5_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "student A: train on labeled 'unlabel1', and then label unlabel2"
      ],
      "metadata": {
        "id": "jQy-yoa6a-aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#설치 라이브러리\n",
        "\n",
        "# colab에서 GPU 사용시, 다음 코드를 먼저 실행해야 sacrebleu 설치 오류 안 남.\n",
        "\n",
        "# import locale\n",
        "# def getpreferredencoding(do_setlocale = True):\n",
        "#     return \"UTF-8\"\n",
        "# locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "\n",
        "#!pip install datasets SentencePiece sacrebleu"
      ],
      "metadata": {
        "id": "jXzjsj1_APdA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#model\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "#for training\n",
        "from time import time\n",
        "\n",
        "#for bleu\n",
        "import sacrebleu"
      ],
      "metadata": {
        "id": "sBUlwwvPbFHM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set up for data"
      ],
      "metadata": {
        "id": "nnbTXKWDaasN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('labeled2')\n",
        "test = pd.read_csv('labeled1')\n",
        "\n",
        "dataB=[] #train data\n",
        "for e,d,g in zip(train.en, train.de, train.gpt):\n",
        "  dataA.append({'en':e, 'de':d, 'gpt': g})\n",
        "\n",
        "dataA=[] #test data\n",
        "for e,d,g in zip(test.en, test.de, test.gpt):\n",
        "  dataB.append({'en':e, 'de':d, 'gpt': g})"
      ],
      "metadata": {
        "id": "qUxfiA1Yateq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #used for checking...\n",
        "\n",
        "# en= ['I am good!' for _ in range(10000)]\n",
        "# de= ['Ich bin gut!' for _ in range(10000)]\n",
        "# gpt= ['I am fine!' for _ in range(10000)]\n",
        "# example= pd.DataFrame({'en':en,'de':de,'gpt':gpt})"
      ],
      "metadata": {
        "id": "c2IrkPdeaTGw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model, tokenizer"
      ],
      "metadata": {
        "id": "Bk2xxGg-IzOo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1GGRFu__zxy",
        "outputId": "76497445-6cc2-4753-e28c-ab0360893752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# evaluation을 하려면 필요한 것 :\n",
        "# dataset = load_dataset(\"bbaaaa/iwslt14-de-en\")\n",
        "# eval= dataset['evaluation']\n",
        "#\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data & split"
      ],
      "metadata": {
        "id": "IKT6ZKlfWakX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 어떻게 data를 select 했는지에 대한 코드\n",
        "\n",
        "# train= dataset['train']\n",
        "# seed_value = 42\n",
        "# train = train.shuffle(seed=seed_value)\n",
        "\n",
        "# train_50= train.select(range(50))          #for baseline\n",
        "# train_250= train.select(range(250))        #for baseline\n",
        "# unlabel1=train.select(range(10000,20000))  #for MCKD\n",
        "# unlabel2=train.select(range(20000,30000))  #for MCKD\n",
        "\n",
        "# n=100\n",
        "# test_n= test.select(range(n))"
      ],
      "metadata": {
        "id": "Kov_u-wBHT6k"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "9egM6XYlWfoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AGIZHKmAZBp",
        "outputId": "81d86a0b-d9a8-4018-a3f7-4ead2f04bee0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "start= time()\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(dataB, batch_size=batch_size)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=3e-4) ##lr from huggingface tip\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    now=time()\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, time: {np.round(now-start,3)//60}분 {np.round((now-start,3)%60)}초\")\n",
        "\n",
        "    for batch in train_loader:\n",
        "        de_inputs = torch.tensor(tokenizer(batch['de'], padding=True).input_ids).to(device)\n",
        "        en_labels = torch.tensor(tokenizer(batch['gpt'], padding=True).input_ids).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=de_inputs, labels=en_labels)\n",
        "        loss = outputs.loss #cpu: 1분에 1배치 (16), cuda: 1배치당 0.1초\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('./fine_tuned_t5_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lZxoYGqACDh",
        "outputId": "594d3b23-5324-49ad-d4c3-ac23cf613258"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, time: 0.0분 5.903초\n",
            "Loss: 1.301015019416809\n",
            "Epoch 2/10, time: 0.0분 16.476초\n",
            "Loss: 0.8126190900802612\n",
            "Epoch 3/10, time: 0.0분 24.027초\n",
            "Loss: 0.5265286564826965\n",
            "Epoch 4/10, time: 0.0분 31.657초\n",
            "Loss: 0.4145004451274872\n",
            "Epoch 5/10, time: 0.0분 39.351초\n",
            "Loss: 0.28334397077560425\n",
            "Epoch 6/10, time: 0.0분 47.098초\n",
            "Loss: 0.20695315301418304\n",
            "Epoch 7/10, time: 0.0분 54.875초\n",
            "Loss: 0.16533568501472473\n",
            "Epoch 8/10, time: 1.0분 2.6430000000000007초\n",
            "Loss: 0.12700384855270386\n",
            "Epoch 9/10, time: 1.0분 10.385000000000005초\n",
            "Loss: 0.08077181875705719\n",
            "Epoch 10/10, time: 1.0분 18.051000000000002초\n",
            "Loss: 0.06468721479177475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for batch processing\n",
        "batch_size = 16\n",
        "test_loader = DataLoader(dataA, batch_size=batch_size) #size: 10000\n",
        "model.eval()\n",
        "print(\"준비됐다옹!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYJ2uhTXAcMd",
        "outputId": "4ea4a9e8-7157-433c-9edd-e39a62f2054b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "냐옹!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference & BLEU"
      ],
      "metadata": {
        "id": "RyOoFqgGYMCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs= []\n",
        "ref=[]\n",
        "i=0\n",
        "\n",
        "start= time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        i+=1\n",
        "        now= time()\n",
        "        print(f\"batch {i} / {n//batch_size+1}, time: {np.round(now-start,3)//60}분 {np.round(now-start,3)%60}초\")\n",
        "\n",
        "        de_inputs = torch.tensor(tokenizer(batch['de'], padding=True).input_ids).to(device)\n",
        "        ref.append(batch['gpt'])\n",
        "        output= model.generate(de_inputs)\n",
        "        for sentence in output:\n",
        "          outputs.append(tokenizer.decode(sentence, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqJ--eNkFfe9",
        "outputId": "33c75b49-7e79-4483-dac3-c04dbaad0b4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 1 / 7, time: 0.0분 0.001초\n",
            "batch 2 / 7, time: 0.0분 0.57초\n",
            "batch 3 / 7, time: 0.0분 1.069초\n",
            "batch 4 / 7, time: 0.0분 1.525초\n",
            "batch 5 / 7, time: 0.0분 1.978초\n",
            "batch 6 / 7, time: 0.0분 2.413초\n",
            "batch 7 / 7, time: 0.0분 2.845초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#output이 아니라 outputs 입니다\n",
        "\n",
        "outputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "sYu7fR-wL5hB",
        "outputId": "48ac6b95-ef8d-46b3-9a6c-c2c0e583f09a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bg: an energy breach is the allerwichtigest.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU score of studentA on dataB (논문에서 필요한 과정은 아님)"
      ],
      "metadata": {
        "id": "p1--feH2eQnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batched list -> list\n",
        "\n",
        "reference = []\n",
        "for sublist in ref:\n",
        "    for item in sublist:\n",
        "        reference.append(item)"
      ],
      "metadata": {
        "id": "z1CnONolM9BR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = sacrebleu.corpus_bleu(outputs, [reference])\n",
        "print(f\"SacreBLEU score: {bleu.score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNk-K0lQM5W9",
        "outputId": "897afdf2-30bb-4e60-96b7-af47e7a6091e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SacreBLEU score: 11.687752266823585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 아래 코드는 되는지 확인 안해봤음..!\n",
        "\n",
        "dataB에 담겼는지 확인하고 내보내기 하세용"
      ],
      "metadata": {
        "id": "ebE-hEJPe_WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## dataA_1를 만들어 내보내기\n",
        "\n",
        "for i in range(10000):\n",
        "  dataA[i]['en_1']= outputs[i]"
      ],
      "metadata": {
        "id": "1_ptYxnCea5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataA[0] #en_1이 담겼는지 확인."
      ],
      "metadata": {
        "id": "EcWxjTk-fMH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataA.to_csv('dataA_1.csv')"
      ],
      "metadata": {
        "id": "-SBV_QBnfNGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}